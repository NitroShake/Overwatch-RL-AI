globalvar define e = 2.718281828;
playervar define reward = 0;

class NeuralNetwork {
    define learningRate = 0.01;
    define w1;
    define w2;   
    define lastLayer1;

    public constructor(define lRate, define weights1, define weights2) {
        learningRate = lRate;
        w1 = weights1;
        w2 = weights2;
    }

    Number feedForward(define inputs) 'test'
    {
        LogToInspector("Feeding forward...");
        define output = 0;
        define layer1 = [];

        //forward all inputs to hidden layer. w2 has one weight per HL node, so it's used for the countof
        for (define i = 0; i < CountOf(w2); i++) {
            for (define j = 0; j < CountOf(inputs); j++) {
                layer1[i] += inputs[j] * w1[j][i];
                //LogToInspector(w1[j][i]);
            }
        }
        //LogToInspector(layer1);

        //activation function on hidden layer
        for (define i = 0; i < CountOf(w2); i++) {
            layer1[i] = tanh(layer1[i]);
        } 

        //LogToInspector(layer1);

        //forward hidden layer to output node
        for (define i = 0; i < CountOf(w2); i++) {
            output += layer1[i] * w2[i];
        }

        lastLayer1 = layer1;

        return output;
    }

    //feedforward optimised for multiple FFs. calculates the non-activated values for the first hidden layer once,
    //then reuses them for each FF, adding on inputs from extraInputs[i] each time.
    //this reduces the multiplications needed for subsequent FFs, saving server resources
    //TODO:: Make extraWeights not required. They're part of the original array anyway, you can do this ^_^
    define optimisedFeedForward(define inputs, define extraInputs, define extraWeights) {
        define layer1 = [];
        define output = 0;

        for (define i = 0; i < CountOf(w2); i++) {
            for (define j = 0; j < CountOf(inputs); j++) {
                layer1[i] += inputs[j] * w1[j][i];
            }
        }

        define outputs = [];

        for (define i = 0; i < CountOf(extraInputs); i++) {
            define hiddenLayer = layer1;
            
            for (define j = 0; j < CountOf(hiddenLayer); j++) {
                for (define k = 0; k < CountOf(extraInputs[i]); k++) {
                    hiddenLayer[j] += extraInputs[i][k] * extraWeights[i][k][j];
                }
            }

            for (define j = 0; j < CountOf(layer1); j++) {
                hiddenLayer[j] = tanh(hiddenLayer[j]);
            } 

            //forward hidden layer to output node
            for (define j = 0; j < CountOf(hiddenLayer); j++) {
                output += hiddenLayer[j] * w2[j];
            }

            outputs = outputs.Append(output);
        }

        return outputs;
    }

    Number tanh(define x) 
    {
        define e2x = RaiseToPower(e, 2*x);
        return (e2x - 1) / (e2x + 1);
    }

    Number tanhDerivative(define x) {
        return 1 - RaiseToPower(tanh(x), 2);
    }

    void backPropagate(define inputs, define actualValue) 'Back-propagation'
    {
        define loss = actualValue - feedForward(inputs);
        LogToInspector("Back-propagating...");
        define hlLosses = [];
        for (define i = 0; i < CountOf(w2); i++) {
            hlLosses = Append(hlLosses, w2[i] * loss * tanhDerivative(lastLayer1[i]));
            w2[i] = w2[i] - (learningRate * lastLayer1[i] * loss);
        }

        for (define i = 0; i < CountOf(hlLosses); i++) {
            for (define j = 0; j < CountOf(inputs); j++) {
                w1[j][i] = w1[j][i] - (learningRate * inputs[j][i] * hlLosses[i]);
            }
        }
    }
}

rule: 'Match Init'
{
    //these two are crucial for performance - server crashes at regular speed, and
    //client crashes if inspector recording is enabled (too many step-throughs)
    SetSlowMotion(33);
    //DisableInspectorRecording();
    
    //skip through game's setup phases. They're unneccesary with AI. Wait times for extra stability
    Wait(5);
    SetMatchTime(0);
    Wait(5);
    SetMatchTime(0);

    //spawn dummy next to slot 1 (teammate)
    CreateDummyBot(Hero.Soldier76, Team.Team1, 0, PlayersInSlot(1));
}

globalvar NeuralNetwork targetNetwork;
globalvar NeuralNetwork passiveNetwork;

rule: 'AI Init'
Player.Slot0
{
    targetNetwork = new NeuralNetwork(0.1, [[0.3, 0.8], [0.6, -0.7], [-0.4, -0.5]], [-0.3, 0.2]);
    LogToInspector(targetNetwork.feedForward([0.36, -1, 0.4]));
    targetNetwork.backPropagate([0.36, -1, 0.4], 5);
    LogToInspector(targetNetwork.feedForward([0.36, -1, 0.4]));
}

rule: 'AI Loop'
Event.OngoingPlayer
Player.Slot0
{
    
}

rule: 'Reward damage'
Event.OnDamageDealt
Player.Slot0
{
    reward += EventDamage();
}

rule: 'Reward kill'
Event.OnElimination
Player.Slot0
{
    reward += 200;
}

rule: 'Reward damage'
Event.OnDamageTaken
Player.Slot0
{
    reward -= EventDamage();
}

rule: 'Reward damage'
Event.OnDeath
Player.Slot0
{
    reward -= EventDamage();
}

rule: 'place heals'
Event.OngoingPlayer
if (Health(EventPlayer()) < 150 && AbilityCooldown(EventPlayer(), Button.Ability2) == 0)
{
    PressButton(EventPlayer(), Button.Ability2);
}