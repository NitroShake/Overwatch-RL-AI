globalvar define e = 2.718281828;
playervar define reward = 0;
globalvar define choiceMaxSteps = 3;
globalvar define choiceStepSize = 6;

playervar define targetPos;
playervar define aimTarget;

class NeuralNetwork {
    define learningRate = 0.01;
    define w1;
    define w2;   
    define lastLayer1;

    public constructor(define lRate, define weights1, define weights2) {
        learningRate = lRate;
        w1 = weights1;
        w2 = weights2;
    }

    Number feedForward(define inputs) 'test'
    {
        LogToInspector("Feeding forward...");
        define output = 0;
        define layer1 = [];

        //forward all inputs to hidden layer. w2 has one weight per HL node, so it's used for the countof
        for (define i = 0; i < CountOf(w2); i++) {
            for (define j = 0; j < CountOf(inputs); j++) {
                layer1[i] += inputs[j] * w1[j][i];
                //LogToInspector(w1[j][i]);
            }
        }
        //LogToInspector(layer1);

        //activation function on hidden layer
        for (define i = 0; i < CountOf(w2); i++) {
            layer1[i] = tanh(layer1[i]);
        } 

        //LogToInspector(layer1);

        //forward hidden layer to output node
        for (define i = 0; i < CountOf(w2); i++) {
            output += layer1[i] * w2[i];
        }

        lastLayer1 = layer1;

        return output;
    }

    //feedforward optimised for multiple FFs. calculates the non-activated values for the first hidden layer once,
    //then reuses them for each FF, adding on inputs from extraInputs[i] each time.
    //this reduces the multiplications needed for subsequent FFs, saving server resources
    define optimisedFeedForward(define inputs, define extraInputs) {
        define layer1 = [];
        define baseWeightI = 0;

        for (define i = 0; i < CountOf(w2); i++) {
            for (define j = 0; j < CountOf(inputs); j++) {
                layer1[i] += inputs[j] * w1[j][i];
                baseWeightI = j;
            }
        }

        LogToInspector("i {0}".Format([baseWeightI]));

        define outputs = [];
        LogToInspector('the layers');
        for (define i = 0; i < CountOf(extraInputs); i++) {
            define output = 0;
            define hiddenLayer = layer1;
            for (define j = 0; j < CountOf(hiddenLayer); j++) {
                for (define k = 0; k < CountOf(extraInputs[i]); k++) {
                    hiddenLayer[j] += extraInputs[i][k] * w1[baseWeightI + 1 + k][j];
                    LogToInspector('multiplying {0} by {1}'.Format([extraInputs[i][k], w1[baseWeightI + 1 + k][j]]));
                }
            }

            for (define j = 0; j < CountOf(layer1); j++) {
                hiddenLayer[j] = tanh(hiddenLayer[j]);
            } 

            LogToInspector('hidden layer: {0} | {1}'.Format([hiddenLayer[0], hiddenLayer[1]]));

            //forward hidden layer to output node
            for (define j = 0; j < CountOf(hiddenLayer); j++) {
                output += hiddenLayer[j] * w2[j];
                LogToInspector('adding {0} * {1} to output'.Format([hiddenLayer[j], w2[j]]));
            }

            outputs = outputs.Append(output);
            LogToInspector('----');
        }

        return outputs;
    }

    Number tanh(define x) 
    {
        define e2x = RaiseToPower(e, 2*x);
        return (e2x - 1) / (e2x + 1);
    }

    Number tanhDerivative(define x) {
        return 1 - RaiseToPower(tanh(x), 2);
    }

    void backPropagate(define inputs, define actualValue) 'Back-propagation'
    {
        define loss = feedForward(inputs) - actualValue;
        LogToInspector("Back-propagating...");
        define hlLosses = [];
        for (define i = 0; i < CountOf(w2); i++) {
            hlLosses = Append(hlLosses, w2[i] * loss * tanhDerivative(lastLayer1[i]));
            w2[i] = w2[i] - (learningRate * lastLayer1[i] * loss);
        }

        for (define i = 0; i < CountOf(hlLosses); i++) {
            for (define j = 0; j < CountOf(inputs); j++) {
                w1[j][i] = w1[j][i] - (learningRate * inputs[j][i] * hlLosses[i]);
            }
        }
    }
}

rule: 'Match Init'
{
    //these two are crucial for performance - server crashes at regular speed, and
    //client crashes if inspector recording is enabled (too many step-throughs)
    SetSlowMotion(33);
    //DisableInspectorRecording();
    
    //skip through game's setup phases. They're unneccesary with AI. Wait times for extra stability
    Wait(5);
    SetMatchTime(0);
    Wait(5);
    SetMatchTime(0);

    //spawn dummy next to slot 1 (teammate)
    CreateDummyBot(Hero.Soldier76, Team.Team1, 0, PlayersInSlot(1));
}

globalvar NeuralNetwork targetNetwork;
globalvar NeuralNetwork passiveNetwork;

rule: 'AI Init'
Player.Slot0
{
    targetNetwork = new NeuralNetwork(0.1, [[0.3, 0.8], [0.6, -0.7], [-0.4, -0.5]], [-0.3, 0.2]);
    LogToInspector(targetNetwork.feedForward([0.36, -1, 0.4]));
    LogToInspector(targetNetwork.optimisedFeedForward([0.36, -1], [[0.4]]));
    define sus = targetNetwork.optimisedFeedForward([0.36], [[-1, 0.4], [0.8, -0.11], [-1, 0.4]]);
    LogToInspector('the susses');
    LogToInspector(sus[0]);
    LogToInspector(sus[1]);
    LogToInspector(sus[2]);
    targetNetwork.backPropagate([0.36, -1, 0.4], -5);
    LogToInspector(targetNetwork.feedForward([0.36, -1, 0.4]));
    LogToInspector(targetNetwork.optimisedFeedForward([0.36, -1], [[0.4]]));
}

//TODO: make this function do literally anything, returning an array [inputs, [extraInputs1, extraInputs2, etc.], targets]
Number[] getInputs(define position) {
    define enemiesInRadius = PlayersWithinRadius(position, 50, Team.All, RadiusLOS.SurfacesAndAllBarriers);
    define inputs = [];
    define extraInputs = [];
    return inputs;
}

//adds numbers to array such that all values become non-negative.
//e.g. [-20, 40, -30, 60] becomes [10, 70, 0, 90]
Number[] numsToAllPositiveNums(define array) {
    define lowestNumber = 0;
    for (define i = 0; i < CountOf(array); i++) {
        lowestNumber = Min(lowestNumber, array[i]);
    }
    for (define i = 0; i < CountOf(array); i++) {
        array[i] = array[i] - lowestNumber;
    }
    return array;
}

Number sumArray(define array) {
    define sum = 0;
    for (define i = 0; i < CountOf(array); i++) {
        sum += array[i];
    }
    return sum;
}

rule: 'AI Loop'
Event.OngoingPlayer
Player.Slot0
{
    define choicePositions = [];
    define choiceTargets = [];
    define choiceInputs = [];
    define choiceValues = [];

    //get the values of every possible state/action
    for (define i = -choiceMaxSteps; i <= choiceMaxSteps; i++) {
        for (define j = -choiceMaxSteps; i <= choiceMaxSteps; i++) {
            define position = NearestWalkablePosition(WorldVectorOf(Vector(0,0,0), EventPlayer(), LocalVector.RotationAndTranslation) + 
                                                      Vector(choiceMaxSteps * choiceStepSize, 0, choiceMaxSteps * choiceStepSize));
            define input = getInputs(position);
            for (define k = 0; k < CountOf(input[1]); i++) {
                choicePositions = Append(choicePositions, position);
                choiceInputs = Append(choiceInputs, Append(input[0], input[1][k]));
            }
            choiceTargets = Append(choiceTargets, [input[2]]);
            define extraInputs = input[1];
            input = input[0];
            define values = targetNetwork.optimisedFeedForward(input, extraInputs);
            choiceValues = Append(choiceValues, [values]);
            Wait(0.016);
        }
    }

    //get the index of the state-action to be chosen.
    define positiveValues = numsToAllPositiveNums(choiceValues);
    define sumValues = sumArray(positiveValues);
    define probability = 0;
    define rand = RandomReal(0, 1);
    define i = -1;
    while (probability < rand) {
        i++;
        probability += positiveValues[i] / sumValues;
    }

    //todo: aim at target, move towards position, add inputs to the tracky thing (you know what i mean)


    //end rule, run again next tick
    Wait(0.016);
    Loop();
}

rule: 'Reward damage'
Event.OnDamageDealt
Player.Slot0
{
    reward += EventDamage();
}

rule: 'Reward kill'
Event.OnElimination
Player.Slot0
{
    reward += 200;
}

rule: 'Reward damage'
Event.OnDamageTaken
Player.Slot0
{
    reward -= EventDamage();
}

rule: 'Reward damage'
Event.OnDeath
Player.Slot0
{
    reward -= EventDamage();
}

rule: 'place heals'
Event.OngoingPlayer
if (Health(EventPlayer()) < 150 && AbilityCooldown(EventPlayer(), Button.Ability2) == 0)
{
    PressButton(EventPlayer(), Button.Ability2);
}