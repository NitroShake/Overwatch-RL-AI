//This is the main file - you'll want to compile using this file (it imports every other one).

import 'pathfinder.del';

globalvar define e = 2.718281828;
playervar define deltaReward = 0;
globalvar define choiceMaxSteps = 3;
globalvar define choiceStepSize = 6;
globalvar define deltaTime = 0;
globalvar define pastInputs = [];
globalvar define pastInputTimes = [];
globalvar define pastInputRewards = [];
globalvar define aiIsRunning = false;
playervar define aimTarget = null;
playervar define dmgPercent = 30; //dmg multiplier (in percent) for primary fire (compensating for near-perfect aim)

playervar define dmgDealtStat = 0;
playervar define healingStat = 0;
playervar define eliminationStat = 0;
playervar define deathStat = 0;
playervar define rocketDmgStat = 0;
playervar define ultimateDmgStat = 0;


globalvar define w1Import1;
globalvar define w1Import2;
globalvar define w2Import;

rule: '>>> Import Weights Part 1 (paste them here!) <<<'
{
    
}

rule: '>>> Import Weights Part 2 (paste them here!) <<<'
{
    
}

rule: '>>> Import Weights Part 3 (paste them here!) <<<'
{
    
}

rule: 'init weights'
{
    Wait(5);
    w1 = [];
    w2 = [];
    if (w1Import1 != null && w1Import2 != null && w2Import != null) {
        w1 = Append(w1, w1Import1);
        w1 = Append(w1, w1Import2);
        w2 = w2Import;
    }
    else {
        //TODO: MAKE SURE THE ARRAY IS ACTUALLY THE SIZE OF THE WEIGHTS
        for (define i! = 0; i < 80; i++) {
            define array = [];
            for (define j! = 0; j < 60; j++) {
                array = Append(array, RandomReal(0, 1));
            }   
            w1 = Append(w1, [array]);
            Wait(0.016);
        }

        for (define i! = 0; i < 60; i++) {
            w2 = Append(w2, RandomReal(0, 1));
            Wait(0.016);
        }
    }
}



//===============================================================================================================
//NEURAL NETWORK BEGINS HERE
//this could be bundled into a class, but classes are merely an abstraction that place their instance's vars into a shared array
//this effectively makes the weights a 3d array, which would kill performance
globalvar define learningRate = 0.1;
globalvar define w1;
globalvar define w2;   
globalvar define lastLayer1;

Number feedForward(define inputs) 'test'
{
    LogToInspector("Feeding forward...");
    define output = 0;
    define layer1 = [];

    //forward all inputs to hidden layer. w2 has one weight per HL node, so it's used for the countof
    for (define i! = 0; i < CountOf(w2); i++) {
        for (define j! = 0; j < CountOf(inputs); j++) {
            layer1[i] += inputs[j] * w1[j][i];
            //LogToInspector(w1[j][i]);
        }
    }
    //LogToInspector(layer1);

    //activation function on hidden layer
    for (define i! = 0; i < CountOf(w2); i++) {
        layer1[i] = tanh(layer1[i]);
    } 

    //LogToInspector(layer1);

    //forward hidden layer to output node
    for (define i! = 0; i < CountOf(w2); i++) {
        output += layer1[i] * w2[i];
    }

    lastLayer1 = layer1;

    return output;
}

//feedforward optimised for multiple FFs. calculates the non-activated values for the first hidden layer once,
//then reuses them for each FF, adding on inputs from extraInputs[i] each time.
//this reduces the multiplications needed for subsequent FFs, saving server resources
define optimisedFeedForward(define inputs, define extraInputs) {
    define layer1 = [];
    define baseWeightI = 0;

    for (define i! = 0; i < CountOf(w2); i++) {
        for (define j = 0; j < CountOf(inputs); j++) {
            layer1[i] += inputs[j] * w1[j][i];
            baseWeightI = j;
        }
    }

    //LogToInspector("i {0}".Format([baseWeightI]));

    define outputs = [];
    LogToInspector('the layers');
    for (define i! = 0; i < CountOf(extraInputs); i++) {
        define output = 0;
        define hiddenLayer = layer1;
        for (define j = 0; j < CountOf(hiddenLayer); j++) {
            for (define k = 0; k < CountOf(extraInputs[i]); k++) {
                hiddenLayer[j] += extraInputs[i][k] * w1[baseWeightI + 1 + k][j];
                //LogToInspector('multiplying {0} by {1}'.Format([extraInputs[i][k], w1[baseWeightI + 1 + k][j]]));
            }
        }

        for (define j! = 0; j < CountOf(layer1); j++) {
            hiddenLayer[j] = tanh(hiddenLayer[j]);
        } 

        //LogToInspector('hidden layer: {0} | {1}'.Format([hiddenLayer[0], hiddenLayer[1]]));

        //forward hidden layer to output node
        for (define j! = 0; j < CountOf(hiddenLayer); j++) {
            output += hiddenLayer[j] * w2[j];
            //LogToInspector('adding {0} * {1} to output'.Format([hiddenLayer[j], w2[j]]));
        }

        outputs = outputs.Append(output);
        //LogToInspector('----');
    }

    return outputs;
}

Number tanh(define x) 
{
    define e2x = RaiseToPower(e, 2*x);
    return (e2x - 1) / (e2x + 1);
}

Number tanhDerivative(define x) {
    return 1 - RaiseToPower(tanh(x), 2);
}

void backPropagate(define inputs, define actualValue) 'Back-propagation'
{
    define loss = feedForward(inputs) - actualValue;
    LogToInspector("Back-propagating...");
    define hlLosses = [];
    for (define i = 0; i < CountOf(w2); i++) {
        hlLosses = Append(hlLosses, w2[i] * loss * tanhDerivative(lastLayer1[i]));
        w2[i] = w2[i] - (learningRate * lastLayer1[i] * loss);
    }

    for (define i = 0; i < CountOf(hlLosses); i++) {
        for (define j = 0; j < CountOf(inputs); j++) {
            w1[j][i] = w1[j][i] - (learningRate * inputs[j][i] * hlLosses[i]);
        }
    }
}
//NEURAL NETWORK ENDS HERE
//==========================================================================

rule: 'Match Init'
{
    CreateHudText(AllPlayers(), '{0} {1} {2}'.Format([ServerLoad(), ServerLoadAverage(), ServerLoadPeak()]),
    null, null, Location.Left, 1, Color.White, Color.Aqua, Color.Aqua, HudTextRev.String, Spectators.VisibleAlways);
    //these two are crucial for performance - server crashes at regular speed, and
    //client crashes if inspector recording is enabled (too many step-throughs)
    SetSlowMotion(33);
    //DisableInspectorRecording();
    
    //skip through game's setup phases. They're unneccesary with AI. Wait times for extra stability
    Wait(5);
    SetMatchTime(0);
    Wait(5);
    SetMatchTime(0);

    //spawn dummy next to slot 1 (teammate)
    CreateDummyBot(Hero.Soldier76, Team.Team1, 0, PlayersInSlot(1)[0]);
}

rule: 'AI Init'
Player.Slot0
Team.Team1
{

}


rule: 'AI Test'
Player.Slot0
Team.Team1
{
    w1 = [[0.3, 0.8], [0.6, -0.7], [-0.4, -0.5]];
    w2 = [-0.3, 0.2];
    LogToInspector(feedForward([0.36, -1, 0.4]));
    LogToInspector(optimisedFeedForward([0.36, -1], [[0.4]]));
    define sus = optimisedFeedForward([0.36], [[-1, 0.4], [0.8, -0.11], [-1, 0.4]]);
    LogToInspector('the susses');
    LogToInspector(sus[0]);
    LogToInspector(sus[1]);
    LogToInspector(sus[2]);
    backPropagate([0.36, -1, 0.4], -5);
    LogToInspector(feedForward([0.36, -1, 0.4]));
    LogToInspector(optimisedFeedForward([0.36, -1], [[0.4]]));
}

//TODO: make this function do literally anything, returning an array [inputs, [extraInputs1, extraInputs2, etc.], targets]
//per teammate/enemy: distance, vertical distance, health, cooldown 1, 2, ult charge
//self: Health, ammo, cooldown rocket/heals, is using ultimate, potential healing?
//target: Health, distance, target hero thingy (5x bools)
Number[] getInputs(define position) {
    define enemiesInRadius = PlayersWithinRadius(position, 50, Team.All, RadiusLOS.SurfacesAndEnemyBarriers);
    define inputs = [];
    define extraInputs = [];
    return inputs;
}

//adds numbers to array such that all values become non-negative.
//e.g. [-20, 40, -30, 60] becomes [10, 70, 0, 90]
Number[] numsToAllPositiveNums(define array) {
    define lowestNumber = 0;
    for (define i! = 0; i < CountOf(array); i++) {
        lowestNumber = Min(lowestNumber, array[i]);
    }
    for (define i! = 0; i < CountOf(array); i++) {
        array[i] = array[i] - lowestNumber;
    }
    return array;
}

Number sumArray(define array) {
    define sum = 0;
    for (define i! = 0; i < CountOf(array); i++) {
        sum += array[i];
    }
    return sum;
}

rule: 'AI Loop'
Event.OngoingPlayer
Player.Slot0
Team.Team1
if (false)
{
    if (MatchRound() < 2 && IsAlive(EventPlayer())) {
        define choicePositions = [];
        define choiceTargets = [];
        define choiceInputs = [];
        define choiceValues = [];

        //get the values of every possible state/action
        for (define i! = -choiceMaxSteps; i <= choiceMaxSteps; i++) {
            for (define j! = -choiceMaxSteps; j <= choiceMaxSteps; j++) {
                define positions = [NearestWalkablePosition(PositionOf(EventPlayer()) + 
                                    Vector(i * choiceStepSize, 0, j * choiceStepSize))];
                for (define k! = -1; k <= 1; k += 2) {
                    define potentialPosition = NearestWalkablePosition(PositionOf(EventPlayer()) + 
                                                Vector(i * choiceStepSize, k * 4, j * choiceStepSize));
                    if (AbsoluteValue(positions[0].Y - potentialPosition.Y) >= 4) {
                        positions = Append(positions, potentialPosition);
                    }
                }

                for (define posI! = 0; posI < CountOf(positions); posI++) {
                    define input = getInputs(positions[posI]);
                    for (define k = 0; k < CountOf(input[1]); k++) {
                        choicePositions = Append(choicePositions, positions[posI]);
                        choiceInputs = Append(choiceInputs, Append(input[0], input[1][k]));
                    }
                    choiceTargets = Append(choiceTargets, input[2]);
                    define extraInputs = input[1];
                    input = input[0];
                    define values = optimisedFeedForward(input, extraInputs);
                    choiceValues = Append(choiceValues, [values]);
                    Wait(0.016);
                }
            }
        }

        //get the index of the state-action to be chosen.
        define positiveValues = numsToAllPositiveNums(choiceValues);
        define sumValues = sumArray(positiveValues);
        define probability = 0;
        define rand = RandomReal(0, 1);
        define actionIndex = -1;
        while (probability < rand) {
            actionIndex++;
            probability += positiveValues[actionIndex] / sumValues;
        }
        Wait(0.016);

        handleBackprop(30);
        
        //set enemy and position targets
        pfDestination = choicePositions[actionIndex];
        aimTarget = choiceTargets[actionIndex];
        if (aimTarget != null) {
            StartFacing(EventPlayer(), aimTarget, 2, Relative.ToWorld, FacingRev.None);
        }
        else {
            StartFacing(EventPlayer(), ThrottleOf(EventPlayer()), 2, Relative.ToWorld, FacingRev.DirectionAndTurnRate);
        }

        pastInputs = Append(pastInputs, choiceInputs[actionIndex]);
        pastInputTimes = Append(pastInputTimes, 0);
        pastInputRewards = Append(pastInputRewards, 0);
        //end rule, run again next tick
        Wait(0.016);
        //todo: if aiIsRunning = true
        //related todo: make aiIsRunning false once the first round is over (only one round will be simulated per match)
        Loop();

    } 
    //else if death
    else if (MatchRound() < 2 && !IsAlive(EventPlayer())) {
        //backprop every input made leading up to death
        handleBackprop(0);
    }
    //else (meaning first round has finished, so the simulation should end)
    else {
        handleBackprop(0);
        finish();
    }
}

void handleBackprop(define inputLifespan) 'Attribute reward and handle Backpropagation' {
    if (CountOf(pastInputs) > 0) {
        define indexesToRemove = [];

        //for each input
        for (define i! = 0; i < CountOf(pastInputs); i++) {
            pastInputTimes[i] += deltaTime;

            //add reward to the input, where 0.9 is the discount rate
            pastInputRewards[i] += deltaReward * RaiseToPower(0.9, pastInputTimes[i]);

            //if the input was made more than inputLifespan secs ago, backpropagate using the value accumulated since then
            if (pastInputTimes[i] >= inputLifespan) {
                backPropagate(pastInputs[i], pastInputRewards[i]);
                indexesToRemove = Append(indexesToRemove, i);
                Wait(0.016);
            }
        }

        //remove the backpropagated inputs
        for (define i! = 0; i < CountOf(indexesToRemove); i++) {
            pastInputs = RemoveFromArrayAtIndex(pastInputs, indexesToRemove[i]);
            pastInputTimes = RemoveFromArrayAtIndex(pastInputTimes, indexesToRemove[i]);
            pastInputRewards = RemoveFromArrayAtIndex(pastInputRewards, indexesToRemove[i]);
        }

        deltaTime = 0;
        deltaReward = 0;
    }
}

void finish() 'Log results at end of match'
{
    //Prepare vars for export/import
    w1Import1 = [];
    w1Import2 = [];
    w2Import = [];
    //TODO: ADJUST FOR ACTUAL NODE/WEIGHT COUNT
    for (define i! = 0; i < 40; i++)  {
        w1Import1 = Append(w1Import1, [w1[i]]);
    }
    for (define i! = 40; i < 80; i++)  {
        w1Import2 = Append(w1Import1, [w1[i]]);
    }
    w2Import = w2;

    define aiSoldier = PlayersOnHero(Hero.Soldier76, Team.Team1);
    define enemySoldier = PlayersOnHero(Hero.Soldier76, Team.Team2);

    LogToInspector('stats formatted as: damage/heals/kills/deaths/ultimatedmg/rocketdmg');
    LogToInspector('AI stats: {0}, {1}, {2}, {3}, {4}, {5}'.Format([
        aiSoldier.dmgDealtStat, aiSoldier.healingStat, aiSoldier.eliminationStat, 
        aiSoldier.deathStat, aiSoldier.ultimateDmgStat, aiSoldier.rocketDmgStat
    ]));
    LogToInspector('AI stats: {0}, {1}, {2}, {3}, {4}, {5}'.Format([
        enemySoldier.dmgDealtStat, enemySoldier.healingStat, enemySoldier.eliminationStat, 
        enemySoldier.deathStat, enemySoldier.ultimateDmgStat, enemySoldier.rocketDmgStat
    ]));
    LogToInspector("Team 1 score: {0}".Format([TeamScore(Team.Team1)]));
    DestroyAllDummyBots();
    SetMatchTime(999);
    SetSlowMotion(10);
}

rule: 'Bot is running'
Event.OngoingPlayer
if (IsDummyBot(EventPlayer()))
{
    aiIsRunning = true;
}

rule: 'Bot start shooting'
Event.OngoingPlayer
if (IsDummyBot(EventPlayer()) && aimTarget != null && IsInLineOfSight(EventPlayer(), aimTarget, BarrierLOS.EnemyBarriersBlock))
{
    StopHoldingButton(EventPlayer(), Button.Ability1);
    StartHoldingButton(EventPlayer(), Button.PrimaryFire);
}

rule: 'Bot stop shooting'
Event.OngoingPlayer
if (IsDummyBot(EventPlayer()) && aimTarget == null)
{
    StopHoldingButton(EventPlayer(), Button.PrimaryFire);
    StartHoldingButton(EventPlayer(), Button.Ability1);
}

rule: 'Bot fire rocket'
Event.OngoingPlayer
if (IsDummyBot(EventPlayer()) && aimTarget != null && AbilityCooldown(EventPlayer(), Button.SecondaryFire) == 0)
{
    PressButton(EventPlayer(), Button.SecondaryFire);
}

rule: 'Bot track damage'
Event.OnDamageDealt
Player.Soldier76
{
    if (EventAbility() == Button.PrimaryFire) {
        dmgDealtStat += EventDamage();
    }
    else if (EventAbility() == Button.SecondaryFire) {
        rocketDmgStat += EventDamage() / (dmgPercent / 100);
    }
    else if (EventAbility() == Button.Ultimate) {
        ultimateDmgStat += EventDamage();
    }
}

rule: 'Bot track heal'
Event.OnHealingDealt
Player.Soldier76
{
    healingStat += EventHealing();
}
rule: 'Bot track elim'
Event.OnElimination
Player.Soldier76
{
    eliminationStat += 1;
}
rule: 'Bot track death'
Event.OnDeath
Player.Soldier76
{
    deathStat += 1;
}

//explaining the next 3 rules:
//The AI has near-perfect aim with primary fire. 
//To offset this, the damage dealt outside of its ultimate (which intentionally gives it perfect aim) is lowered to dmgPercent.
//When other abilities (e.g., rocket) hits, the AI's aim is imperfect, so extra damage is dealt to bring the damage dealt back up.
rule: 'AI dmg adjust for ult'
Event.OngoingPlayer
if (IsDummyBot(EventPlayer()) && IsUsingUltimate(EventPlayer()))
{
    SetDamageDealt(EventPlayer(), 100);
}

rule: 'AI dmg adjust for non-ult'
Event.OngoingPlayer
if (IsDummyBot(EventPlayer()) && !IsUsingUltimate(EventPlayer()))
{
    SetDamageDealt(EventPlayer(), dmgPercent);
}

rule: 'AI deal extra damage to offset nerf'
Event.OnDamageDealt
if (IsDummyBot(EventPlayer()) && EventAbility() == Button.SecondaryFire)
{
    Damage(Victim(), EventPlayer(), EventDamage() / (dmgPercent / 100));
}

rule: 'deltaTime update'
Event.OngoingGlobal
if (aiIsRunning)
{
    deltaTime += 0.01666;
    Wait(0.01666);
}

rule: 'Reward damage'
Event.OnDamageDealt
Player.Slot0
Team.Team1
{
    deltaReward += EventDamage();
}

rule: 'Reward kill'
Event.OnElimination
Player.Slot0
Team.Team1
{
    deltaReward += 200;
}

rule: 'Reward damage'
Event.OnDamageTaken
Player.Slot0
Team.Team1
{
    deltaReward -= EventDamage();
}

rule: 'Reward damage'
Event.OnDeath
Player.Slot0
Team.Team1
{
    deltaReward -= EventDamage();
}

rule: 'place heals'
Event.OngoingPlayer
if (Health(EventPlayer()) < 150 && AbilityCooldown(EventPlayer(), Button.Ability2) == 0)
{
    PressButton(EventPlayer(), Button.Ability2);
}